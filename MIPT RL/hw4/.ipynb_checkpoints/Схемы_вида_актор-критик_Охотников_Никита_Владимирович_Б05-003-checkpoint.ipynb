{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация алгоритма Advantage-Actor Critic (A2C) (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания (сразу жёсткий): 26 апреля, 23:59 UTC+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Охотников Никита Владимирович, Б05-003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе Вам предстоит реализовать алгоритм `Advantage Actor Critic`, обучаемый на батче из сред `Atari 2600`, работающих параллельно.\n",
    "\n",
    "Для начала будут использованы обёртки сред, реализованные в файле `atari_wrappers.py`. Эти обёртки предварительно обрабатывают наблюдения (производят преобразования размера, цвета фрейма, взятия максимума между фреймами, пропускают часть фреймов и сводят несколько фреймов в один большой) и вознаграждения. Некоторые обёртки помогают автоматически перезапустить среду и присвоить переменной `done` значение `True` в случае смерти агента. Файл `env_batch.py` включает в себя реализацию класса `ParallelEnvBatch`, позволяющего запускать несколько сред параллельно. Для создания (инициализации) среды можно воспользоваться функцией `nature_dqn_env`. Обратите внимание, что в случае использования `PyTorch` (https://pytorch.org/) без `tensorboardX` (https://github.com/lanpa/tensorboardX) потребуется самостоятельно реализовать обёртку среды, которая будет логрировать **исходные** суммарные награды, которые *исходная* среда возвращает, и переопределить реализацию функции `nature_dqn_env`. То есть настоятельно рекомендуется применить `tensorboardX`.\n",
    "\n",
    "Псевдокод алгоритма `Advantage Actor Critic (A2C)` приведён в **Разделе 5.2.5 (Алгоритм 20)** конспекта лекций: https://arxiv.org/pdf/2201.09746.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты в данной работе используют Python версию библиотеки [OpenCV](https://pypi.org/project/opencv-python/). Для запуска сред ATARI понадобится библиотека [The Arcade Learning Environment](https://github.com/Farama-Foundation/Arcade-Learning-Environment), а также библиотека [Shimmy](https://pypi.org/project/Shimmy/). Может потребоваться установка `ffmpeg` кодека. Для Unix-based операционной системы с пакетным менеджером APT может потребоваться следующая последовательность команд:\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install pyglet pyvirtualdisplay opencv-python tqdm numpy moviepy gymnasium[atari]\n",
    "pip install gymnasium[accept-rom-license] #run 'pip install autorom; AutoROM --accept-license' if this fails\n",
    "```\n",
    "Для MacOS систем рекомендуется установить репозиторий [TFM](https://github.com/serrodcal-MII/TFM/tree/main) и выполнить инструкции из [README.md](https://github.com/serrodcal-MII/TFM/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from atari_wrappers import nature_dqn_env\n",
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nenvs = 16\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, summaries = \"Tensorboard\")\n",
    "                                                    # nenvs -- количество параллельно запущенных сред\n",
    "                                                    # данный параметр можно варьировать для баланса\n",
    "                                                    # производительность итерации/надёжность Монте-Карло оценок\n",
    "                                                    # помните: при уменьшении nenvs, возможно, придётся\n",
    "                                                    # увеличить количество итераций оптимизации\n",
    "\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs, _ = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет реализация модели, которая выводит логиты для категориального распределения на действия и оценку на значения $V$-функции ценности. Рекомендуется использовать архитектуру модели, представленной в публикации в журнале [Nature](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) со следующей модификацией: вместо одного выходного слоя нужно сделать два слоя, принимающих в качестве входа выход предшествующего скрытого слоя. **Обратите внимание**, данная модель отличается от модели, предложенной в домашней работе по DQN. Рекомендуется использовать ортогональную инициализацию с параметром $\\sqrt{2}$ для ядер свёрток и инициализировать смещения нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    общий принцип использования данной функции:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    для вычисления размерности входа полносвязного слоя\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size(0), -1)\n",
    "\n",
    "\n",
    "class NatureModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bsize, nchannels, height, width, n_actions, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=nchannels, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(height, 8, 4), conv2d_size_out(width, 8, 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 4, 2), conv2d_size_out(cur_w, 4, 2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 3, 1), conv2d_size_out(cur_w, 3, 1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = torch.nn.Linear(in_features=64 * cur_h * cur_w, out_features=512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc_logits = torch.nn.Linear(in_features=512, out_features=n_actions)\n",
    "        self.fc_values = torch.nn.Linear(in_features=512, out_features=1)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv1.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv2.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv2.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv3.weight.data, gain=np.sqrt(2.))\n",
    "        self.conv3.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc1.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc1.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_logits.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc_logits.bias.data.fill_(.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_values.weight.data, gain=np.sqrt(2.))\n",
    "        self.fc_values.bias.data.fill_(.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            x = torch.tensor(np.array(state), device=self.device, dtype=torch.float)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        logits = self.fc_logits(x)\n",
    "        value = self.fc_values(x)\n",
    "        \n",
    "        x.cpu()\n",
    "        del x\n",
    "        \n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам также потребуется определить и использовать политику, которая будет использовать модель выше. В то время как модель вычисляет логиты для всех действий сразу и оценку функции ценности, политика будет сэмплировать действия, а также будет вычислять их логарифм правдоподобия. Метод `Policy.act` должен возвращать словарь всех массивов, требуемых для взаимодействия со средой и обучения модели. Обратите внимание, что действия должны быть формата `Numpy.ndarray`, в то время как другие тензоры должны быть в формате, определяемом библиотекой глубокого обучения (`Torch.tensor`, например)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        # <Реализуйте политику посредством прямого прохода модели, сэмплирования действий и вычисления их логарифмов правдоподобия>\n",
    "        # Должен возвращать dict с ключами ['actions', 'logits', 'log_probs', 'values'].\n",
    "\n",
    "        logits, values = self.model(inputs)\n",
    "        categorical_distr = Categorical(logits=logits)\n",
    "        actions = categorical_distr.sample()\n",
    "        log_probs = categorical_distr.log_prob(actions)\n",
    "        distr_entropy = categorical_distr.entropy()\n",
    "        return {\n",
    "            \"actions\": actions,\n",
    "            \"logits\": logits, \n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"distr_entropy\": distr_entropy\n",
    "        }\n",
    "        \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее требуется передать среду и политику в исполнитель `EnvRunner`, который собирает частичные траектории из среды. Класс `EnvRunner` уже реализован за Вас.\n",
    "\n",
    "Данный исполнитель взаимодействует со средой заданное количество шагов и возвращает словарь, содержащий ключи:\n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* и другие ключи, определённые в `Policy`\n",
    "\n",
    "по каждому из этих ключей содержится Python `list` соответствующих результатов взаимодействий со средой указанной длины $T$ &mdash; размера частичной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы обучить часть модели, которая предсказывает ценности состояний, требуется вычислить целевые значения ценностей. В инстанцию класса `EnvRunner` можно подать при создании по аргументу `transforms` список вызываемых объектов (\"функций\"), которые последовательно будут применяться к частичным траекториям после сбора самих траекторий. Следовательно, требуется реализовать и использовать вызываемый (с определённым методом `__call__`) класс `ComputeValueTargets`. Формула для вычисления целевых значений ценности простая:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "Однако, не забудьте в реализации использовать `trajectory['resets']` флаги для проверки того, следует ли добавить целевые значения ценности на следующем шаге при вычислении целевых значений ценности на текущем шаге. У вас также имеется доступ к `trajectory['state']['latest_observation']` для получения последнего наблюдения в частичной траектории &mdash; $s_{t+T+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99, device=torch.device('cuda:0')):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        # Данный метод должен модифицировать траекторию trajectory на месте через добавление\n",
    "        # итеративно заполненного списка по ключу 'value_targets'.\n",
    "        # <Вычислите целевые значения ценности для данных частичных траекторий>\n",
    "        rewards = torch.tensor(trajectory[\"rewards\"],\n",
    "                               dtype=torch.float32).to(self.device)\n",
    "        resets = torch.tensor(trajectory[\"resets\"]).to(self.device)\n",
    "        values = torch.stack(trajectory['values']).to(self.device).squeeze()\n",
    "        \n",
    "        value_targets = deepcopy(rewards)\n",
    "        T = trajectory['state']['env_steps']\n",
    "        # print(resets, \"\\n\\n\", ~resets, \"\\n\\n\")\n",
    "        # print(resets[-1], \"\\n\\n\", ~resets[-1], \"\\n\\n\")\n",
    "        # print(resets.shape, values.shape)\n",
    "        value_targets[-1] = values[-1] * (~resets[-1]) + rewards[-1] * resets[-1]\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            value_targets[t] += value_targets[t+1] * self.gamma * (~resets[t])\n",
    "\n",
    "        trajectory['value_targets'] = value_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После вычисления целевых значений ценности требуется преобразовать списки результатов взаимодействия со средой в тензоры с первой компонентой размерности `batch_size`, равной призведению `T * nenvs`, то есть требуется свести в одну компоненту первые две компоненты размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\"\n",
    "    Сращивает первые две оси, обычно отвечающие за время и за инстанцию среды, соответственно.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Модификация траектории на месте. \n",
    "        trajectory[\"rewards\"] = torch.flatten(torch.tensor(trajectory[\"rewards\"], \n",
    "                                                           dtype=torch.float32, device=device), 0, 1) \n",
    "        trajectory[\"values\"] = torch.flatten(torch.stack(trajectory['values']).to(device), 0, 1) \n",
    "        trajectory[\"value_targets\"] = torch.flatten(trajectory[\"value_targets\"], 0, 1) \n",
    "        trajectory[\"resets\"] = torch.flatten(torch.tensor(trajectory[\"resets\"], device=device), 0, 1) \n",
    "        trajectory[\"log_probs\"] = torch.flatten(torch.stack(trajectory[\"log_probs\"]).to(device), 0, 1) \n",
    "        trajectory[\"distr_entropy\"] = torch.flatten(torch.stack(trajectory[\"distr_entropy\"]).to(device), 0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 617 # на своё усмотрение можно выбрать другой seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = NatureModel(*obs.shape, n_actions, device)\n",
    "model.to(device)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=7, # nsteps -- длина частичной траектории\n",
    "                                          # уменьшение nsteps может привести к вынужденному увеличению\n",
    "                                          # количества итераций оптимизации\n",
    "                   transforms=[ComputeValueTargets(device=device),\n",
    "                               MergeTimeBatch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время реализовать сам алгоритм Advantage-Actor Critic (A2C). Его псевдокод можно посмотреть в [конспектах лекций (Раздел 5.2.5)](https://arxiv.org/pdf/2201.09746.pdf), в публикции [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) и в [лекции](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) Сергея Левина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=.25,\n",
    "               entropy_coef=1e-2,\n",
    "               max_grad_norm=.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "        # Тут нужно вычислить Advantages. \n",
    "        return -(trajectory[\"log_probs\"] * (trajectory['value_targets'] - trajectory['values']).detach()).mean()\n",
    "\n",
    "    def value_loss(self, trajectory):\n",
    "        return F.mse_loss(trajectory['values'].squeeze(), trajectory['value_targets'].detach().squeeze())\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        value_loss = self.value_loss_coef * self.value_loss(trajectory)\n",
    "        policy_loss = self.policy_loss(trajectory)\n",
    "        entropy_loss = self.entropy_coef * trajectory[\"distr_entropy\"].mean() \n",
    "\n",
    "        self.value_loss_value = value_loss.detach()\n",
    "        self.policy_loss_value = policy_loss.detach()\n",
    "        self.entropy_loss_value = entropy_loss.detach()\n",
    "\n",
    "        return value_loss + policy_loss - entropy_loss\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss_value = self.loss(trajectory)\n",
    "        loss_value.backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(optimizer.param_groups[0][\"params\"], self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss_value.detach(), grad_norm.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно непосредственно обучить Вашу модель. С разумно подобранными гиперпараметрами обучение на одной GTX1080 на протяжении 10 миллионов шагов суммарно со всех батчированных сред (что переводится примерно в 5 часов работы) должно быть возможно достигнуть *среднюю исходную награду за 100 последних эпизодов* (значение переменной в `Tensorboard` по ключу `reward_mean_100`, усреднение берётся по 100 последним эпизодам в каждой среде в батче) **не меньше 600**. Это и будет считаться успешным результатом работы алгоритма `A2C`.\n",
    "\n",
    "**Внимание!** При *коректной* имплементации алгоритма *обоснованное* преодоление порога **400** по `reward_mean_100` *транслируется в* **8 баллов за задание**, *обоснованное* преодоление порога **600** по `reward_mean_100` на *корректно* написанном алгоритме обучения `A2C` *транслируется в* **10 баллов за задание**.\n",
    "\n",
    "Вам так же, по возможности, рекомендуется отобразить данную величину относительно `runner.step_var` &mdash; количества взаимодействий со всеми средами. Также очень рекомендуется предоставить графики следующих показателей (полезно для отладки кода):\n",
    "* [Коэффициент детерминации](https://en.wikipedia.org/wiki/Coefficient_of_determination) между целевыми значениями ценности и их предсказаниями\n",
    "* Энтропия политики $\\pi$\n",
    "* Функция потерь ценности (Value loss)\n",
    "* Функция потерь политики (Policy loss)\n",
    "* Целевые значения ценности (Value targets)\n",
    "* Предсказания значений ценности (Value predictions)\n",
    "* Норма градиента\n",
    "* Advantages\n",
    "* Общая функция потерь (A2C loss)\n",
    "\n",
    "В качестве оптимизатора рекомендуется взять метод [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) с [линейным убыванием шага](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html), начиная с 7e-4 до 0, константой сглаживания (alpha в PyTorch и decay в TensorFlow), равной 0.99 и epsilon, равным 1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T01:43:12.609689Z",
     "start_time": "2024-04-16T01:43:12.605243Z"
    }
   },
   "source": [
    "Запуск Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-990f6818aa2a3eb8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-990f6818aa2a3eb8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "alpha=0.99\n",
    "eps=1e-5\n",
    "\n",
    "# lr = 3e-4\n",
    "# betas = (0.88, 0.98)\n",
    "# eps = 1e-7\n",
    "\n",
    "\n",
    "n_steps = 5000000\n",
    "nsteps = 40\n",
    "\n",
    "\n",
    "model = NatureModel(*obs.shape, n_actions, device).to(device)\n",
    "\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, \n",
    "                   policy, \n",
    "                   nsteps=nsteps, \n",
    "                   transforms=[ComputeValueTargets(device=device), MergeTimeBatch()])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                lr=lr, alpha=alpha, eps=eps)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), \n",
    "#                              lr=lr, betas=betas, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                                              lr_lambda=lambda step: (n_steps - step) / n_steps)\n",
    "\n",
    "batch_size = nenvs * nsteps\n",
    "\n",
    "writer = SummaryWriter('./logs/run-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c502ba26698844ff83e5e342a569708f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikita\\AppData\\Local\\Temp\\ipykernel_13332\\4092592334.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  rewards = torch.tensor(trajectory[\"rewards\"],\n",
      "C:\\Users\\Nikita\\AppData\\Local\\Temp\\ipykernel_13332\\4092592334.py:14: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  resets = torch.tensor(trajectory[\"resets\"]).to(self.device)\n",
      "C:\\Users\\Nikita\\AppData\\Local\\Temp\\ipykernel_13332\\4114646125.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  trajectory[\"resets\"] = torch.flatten(torch.tensor(trajectory[\"resets\"], device=device), 0, 1)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a2c = A2C(\n",
    "    policy,\n",
    "    optimizer,\n",
    "    value_loss_coef=1.5e-1,\n",
    "    entropy_coef=5e-3,\n",
    "    max_grad_norm=5)\n",
    "\n",
    "n_iters = n_steps // batch_size\n",
    "eval_step = 100\n",
    "\n",
    "for it in tqdm(range(n_iters)):\n",
    "    trajectory = runner.get_next()\n",
    "    loss_value, grad_norm = a2c.step(trajectory)\n",
    "\n",
    "    if it % eval_step == 0:\n",
    "        writer.add_scalar(\"value_loss\", a2c.value_loss_value.cpu(), it)\n",
    "        writer.add_scalar(\"policy_loss\", a2c.policy_loss_value.cpu(), it)\n",
    "        writer.add_scalar(\"entropy_loss\", a2c.entropy_loss_value.cpu(), it)\n",
    "        writer.add_scalar(\"total_loss\", loss_value.cpu(), it)\n",
    "        writer.add_scalar(\"grad_norm\", grad_norm.cpu(), it)\n",
    "        \n",
    "    scheduler.step()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перебор параметров, оптимизаторов, изменение количество шагов и поиск ошибок не дал результата -- mean_100_reward лишь пару раз превышал 200 в лучших попытках. Имплементацию перечитал несколько раз, не понятно почему не учится, но не учится. Лоссы ведут себя разумно, в ноль не падают, в бесконечность не улетают. Норма градиента в зависимости от попытки колеблется вокруг некоторого среднего в диапазоне 0.03 -- 0.8, а значит обучение идет. По причине того, что обучить нормально не вышло, не вижу смысла прикреплять скриншоты и видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, clip_reward=False, summaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lives = 3\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Играем n_games игр до конца.\n",
    "    В случае жадной политики, выбираем действия как argmax(qvalues).\n",
    "    Возвращаем среднюю награду.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            output = agent.act([s])\n",
    "            action = output['logits'].argmax(dim=-1).item() if greedy else output['actions'][0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/False\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=False) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'False').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/True\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=True) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'True').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информация об обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикрепление скриншотов графиков обучения модели в `Tensorboard` ниже является обязательным. Для доступа к `Tensorboard` запустите из командной строки в одной директории с данным ноутбуком следующую команду:\n",
    "```\n",
    "tensorboard --logdir logs --port 6006\n",
    "```\n",
    "В результате вывод в командную строку укажет, по какому адресу можно подсоединиться к инстанции `Tensorboard`, например, по адресу `http://localhost:6006/`. Оттуда можно и сделать скриншоты, демонстрирующие результаты обучения модели. Сами скриншоты с именем файла `image_name_x.png` для удобства лучше сохранить в директорию `./img`, откуда можно легко их прикреплять в `Markdown-клетках` ниже по команде со следующей конструкцией:\n",
    "```\n",
    "<img src=./img/image_name_x.png width=640>\n",
    "```\n",
    "Тут также требуется подписать изображения и дать небольшой комментарий по каждому скриншоту, что на нём описано.\n",
    "\n",
    "**Внимание!** В случае перезапуска процедуры обучения модели рекомендуется удалить директорию `./logs` вместе с её содержимым перед непосредственным перезапуском, чтобы не испортить отображающиеся графики в `Tensorboard`.\n",
    "\n",
    "**Совет.** При работе в Google Colab можно просто скачать директорию `./logs` и уже локально запустить `Tensorboard` для снятия скриншотов. Также можно обученного агента сохранить, скачать и локально на cpu запустить для записи роликов (для этого понадобится самостоятельно прописать код сохранения и загрузки модели в ноутбук из [файла](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "**Внимание!** Посылку для сдачи задания требуется оформить в виде `.zip` архива, в котором будут *данный ноутбук*, использованные для его работы *скрипты*, *директории* `./videos`, `./logs` и `./img` с содержимым. Только так и не иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вставьте в данную ячейку свой ответ, подкреплённый скриншотами__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "015ef9e103fd439b9d1c81abcb1a02cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3bd43bec84874642a7751f9a471cdaa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4121e9ce761842f8ad08d9adabef6907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "418c9c705341404b86cad310c61ce8fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f1d57d245e24b369a70b6846440f901": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "652de4e01a024ae58558e5eb31c251f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa6313910ece40db911899dbdf103694": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4121e9ce761842f8ad08d9adabef6907",
       "style": "IPY_MODEL_3bd43bec84874642a7751f9a471cdaa6",
       "value": " 78%"
      }
     },
     "aebc530dc8284434ba58238acdaaa36d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c502ba26698844ff83e5e342a569708f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_aa6313910ece40db911899dbdf103694",
        "IPY_MODEL_ed6f44b1532f482e9132c48f3c9856ae",
        "IPY_MODEL_e5873de6cb374961b65a16b977081c25"
       ],
       "layout": "IPY_MODEL_652de4e01a024ae58558e5eb31c251f3"
      }
     },
     "e5873de6cb374961b65a16b977081c25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_015ef9e103fd439b9d1c81abcb1a02cd",
       "style": "IPY_MODEL_418c9c705341404b86cad310c61ce8fb",
       "value": " 6109/7812 [4:35:59&lt;1:22:59,  2.92s/it]"
      }
     },
     "ed6f44b1532f482e9132c48f3c9856ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_aebc530dc8284434ba58238acdaaa36d",
       "max": 7812,
       "style": "IPY_MODEL_5f1d57d245e24b369a70b6846440f901",
       "value": 6109
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
